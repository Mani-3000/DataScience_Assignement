{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3106b333-9531-4bc6-bd3b-d3c8fc0cbc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1:\n",
      "Mean Squared Error: 2096851.1400177048\n",
      "R-squared: 0.8428476112018002\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(r\"C:\\Users\\admin\\Downloads\\MLR\\MLR\\ToyotaCorolla - MLR.csv\")\n",
    "\n",
    "# Encode categorical features\n",
    "le = LabelEncoder()\n",
    "data['Fuel_Type'] = le.fit_transform(data['Fuel_Type'])\n",
    "data['Automatic'] = le.fit_transform(data['Automatic'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = data.drop('Price', axis=1)\n",
    "y = data['Price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model 1: All Features\n",
    "model1 = LinearRegression()\n",
    "model1.fit(X_train, y_train)\n",
    "y_pred1 = model1.predict(X_test)\n",
    "\n",
    "# Model 2: Feature Selection\n",
    "X_train_reduced = X_train[['Age_08_04', 'KM', 'HP', 'cc']]\n",
    "X_test_reduced = X_test[['Age_08_04', 'KM', 'HP', 'cc']]\n",
    "model2 = LinearRegression()\n",
    "model2.fit(X_train_reduced, y_train)\n",
    "y_pred2 = model2.predict(X_test_reduced)\n",
    "\n",
    "# Model 3: Feature Transformation\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "model3 = LinearRegression()\n",
    "model3.fit(X_train_poly, y_train)\n",
    "y_pred3 = model3.predict(X_test_poly)\n",
    "\n",
    "# Evaluate models\n",
    "mse1 = mean_squared_error(y_test, y_pred1)\n",
    "r2_1 = r2_score(y_test, y_pred1)\n",
    "print(\"Model 1:\")\n",
    "print(\"Mean Squared Error:\", mse1)\n",
    "print(\"R-squared:\", r2_1)\n",
    "\n",
    "# Apply Lasso and Ridge Regression\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "lasso_reg.fit(X_train, y_train)\n",
    "y_pred_lasso = lasso_reg.predict(X_test)\n",
    "\n",
    "ridge_reg = Ridge(alpha=1.0)\n",
    "ridge_reg.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge_reg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d51d188-4968-4e6a-a228-50d5c5a23911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normalization scales data to a fixed range (0,1), while Standardization transforms data to have zero mean and unit variance.\n",
      "\n",
      "Standardization is useful for regression models to ensure features contribute equally and improve numerical stability.\n",
      "\n",
      "Techniques to address multicollinearity:\n",
      "1. Variance Inflation Factor (VIF) to detect high correlation between features.\n",
      "2. Removing correlated features based on a correlation matrix.\n",
      "3. Principal Component Analysis (PCA) to reduce dimensionality.\n",
      "4. Ridge Regression (L2 Regularization) to shrink coefficients.\n",
      "5. Lasso Regression (L1 Regularization) to eliminate some features.\n"
     ]
    }
   ],
   "source": [
    "# Explanation of Normalization & Standardization\n",
    "print(\"\\nNormalization scales data to a fixed range (0,1), while Standardization transforms data to have zero mean and unit variance.\")\n",
    "print(\"\\nStandardization is useful for regression models to ensure features contribute equally and improve numerical stability.\")\n",
    "\n",
    "# Explanation of Multicollinearity Handling\n",
    "print(\"\\nTechniques to address multicollinearity:\")\n",
    "print(\"1. Variance Inflation Factor (VIF) to detect high correlation between features.\")\n",
    "print(\"2. Removing correlated features based on a correlation matrix.\")\n",
    "print(\"3. Principal Component Analysis (PCA) to reduce dimensionality.\")\n",
    "print(\"4. Ridge Regression (L2 Regularization) to shrink coefficients.\")\n",
    "print(\"5. Lasso Regression (L1 Regularization) to eliminate some features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80512e7a-b738-46ea-b637-b5ca38cbfd21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
